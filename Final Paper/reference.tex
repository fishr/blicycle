\documentclass[aps,twocolumn,secnumarabic,balancelastpage,amsmath,amssymb,nofootinbib]{revtex4-1}

\usepackage{color}         % produces boxes or entire pages with colored backgrounds
\usepackage{graphics}      % standard graphics specifications
\usepackage[pdftex]{graphicx}      % alternative graphics specifications
\usepackage{epsf}          % old package handles encapsulated post script issues
\usepackage{bm}            % special 'bold-math' package
%\usepackage{asymptote}     % For typesetting of mathematical illustrations
\usepackage{thumbpdf}
\usepackage{algorithm2e}

\begin{document}

\title{Automatic Music Genre Categorization using Multiclass Classification}
\author         {Steve Levine}
\date{\today}
\affiliation{6.867 Machine Learning Final Project}

\maketitle

\section{Project Overview}
For my 6.867 final project, I attempt to use various machine learning techniques to automatically categorize the genre of music. Using Java-code that I wrote, feature vectors relating to various frequency characteristics of the music (ex., bass, middle, and high frequencies, as well as rhythm) are computed. The songs are then sorted into one of five genres (Hip Hop, Electronic, Rock, Jazz and Classical) via two different multiclass classifiers: an off-the-shelf SVM classifier developed at Cornell University, and a Naive Bayes classifier that I wrote in Python. I compare the performance of these classifiers, as well as discuss the tunable parameters of the algorithms. I found that the SVM and Naive Bayes classifiers both yielded comparable results, correctly classifying songs from a test set approximately 65\% of the time (which is better than the guessing ratio of 20\%).

The most significant aspects of this project relating to machine learning are: 1.) The design and implementation of relevant feature vectors for classifying music, 2.) The tuning and analysis of the SVM package, 3.) The implementation of a Naive Bayes classifier, and 4.) The comparison of the SVM classifier to the Naive Bayes classifier.

\section{Feature Vectors for Classifying Music}
\subsection*{Differences between Music Genres}
Developing useful features for classifying music is not a simple task. Music is represented digitally as a discretized, sampled wave form. However, this representation is not particularly useful for classifying music genre, as it is not representative of the psychoacoustics behind how the human ear works. Instead of working in the time domain, humans hear sounds and music in the frequency domain, having a keen ability to detect the difference between low frequencies (such as the deep bass in a hip hop song), mid-range frequencies (talking, singing, saxophones), and high-pitched frequencies (sharp claps, snaps, etc.). This justifies using feature vectors that are computed from the frequency-domain representation of music rather than the raw, time-domain representation stored on computers.

\RestyleAlgo{algoruled}
\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\mathcal{C}_{monitor}$ - a list of currently monitored causal links, and $a$ - the action being started}
\Begin{
	$\textsc{CheckPreconds}(a)$ \;
	$\mathcal{C}_{monitor}.\textsc{AppendAll}(\textsc{CausalStart}(a))$ \;
}
\caption{\textsc{ActionStart}\label{IR}}
\end{algorithm}

Different music genres have different frequency characteristics. For example, hip hop is often associated with deep, rhythmically repetitive bass kicks. Electronic/techno music is similar, and is often even more consistent in its bass-kick rhythm (typically all quarter-notes). Classical music on the other hand often has much less bass, and is generally "softer" than hip hop even after normalization. Classical music typically does not have the repetitive and heavily pronounced rhythms also found in hip hop and electronic music. Jazz and Rock music fall somewhere in the middle, they have moderate volume, and often do contain repetitive rhythms (though often with substantially less bass).

I tried to design my feature vectors to take these differences into account as much as possible. By really pronouncing these differences, I hoped to create useful feature vectors that could separate the different genres of music very well.

\subsection*{Implementation of Features}
In this section, I will describe in detail the 34-dimensional feature vectors that I use, and how I compute them. 

First, my code divides up the discretized waveform into overlapping windows of 2048 samples. It then computes a Fast Fourier Transform on each of these time-domain chunks, resulting in descriptions of the frequencies present at that time in the music. This allows for the analysis of the bass, mids, and highs during that window of the song. Since an FFT is computed for each window, the evolution of frequency characteristics in the music as a function of time can be deduced.

By summing over the FFT values in certain frequency ranges in any one of the FFT's, it is possible to effectively measure how much bass, mids, or highs are present at that particular instant. My code does exactly this, summing over the range 20-80 Hz for bass, 200-2,000 Hz for the mids, and 6,000-20,000 Hz for the highs.

\begin{figure}
\includegraphics[scale=0.15]{Electronic.png}
\caption{Features of an electronic song. Note the extremely constant bass rhythm (lower left plot).}
\label{fig:ElectronicDist}
\end{figure}

\begin{figure}
\includegraphics[scale=0.15]{Classical.png}
\caption{Features of a classical piece. Note the overall lower levels - especially with bass - and the lack of an extremely consistent, thumping rhythm (no peaks in bottom left plot).}
\label{fig:ClassicalDist}
\end{figure}

Since these three values change over time, we may calculate their distributions with respect to time. By computing this distribution over the entire song, we can get a description of what the different frequency ranges are doing throughout the song. My software computes these distributions, in the form of histograms, two examples of which can be seen in Figure \ref{fig:ElectronicDist} (electronic) and Figure \ref{fig:ClassicalDist} (classical). For each of the plots in each Figure, the horizontal axis represents the measured level of bass, mids, or highs, and they vertical axis represents a relative amount measuring how often these different loudnesses occur relative to each other. Please note that the histograms have very large bin sizes - there are effectively four different loudnesses along the horizontal axis that are measured. This is because the direct values at each of these bins are used in my feature vectors, and we are only interested in rough, aggregate loudness information (how much the specific loudness of 63 occurs relative to 82 is too fine-grained). The average, peak, spread, and max values are also computed with each distribution. 

32 of the 34 features are taken directly from these distributions. As noted above, each histogram contains four bins measuring the relative frequency of different loudnesses in the bass, mids, and highs. Each of these measurements are decimal values in the range of 0, 1 and are used in the feature vectors. Additionally, the average, peak, spread, and max values are used. This yields 8 features for each of bass, mids, and highs. The fourth plot show in each of the Figures represents the overall loudness of the song (over all frequencies). 8 more features are derived from this distribution analogous to those of the bass, mids, and highs, accounting for 32 of the 34 features.

The final two features make an attempt to measure the rhythm of the song. Please note however that the concept of rhythm is hard to define rigorously, and even harder to compute quantitatively from a song, so my code uses a rough heuristic. I wanted a feature that would allow me to distinguish between the otherwise very similar genres of electronic and hip hop. Electronic music (techno and house in particular) tends to have near-constant time differences between consecutive bass hits, whereas this is often not the case in hip hop. Therefore, my code attempts to determine precisely when bass hits occur, and then measures the time difference between consecutive bass hits. I will refrain from discussing the specific signal processing details surrounding such computations, as they are a bit involved and irrelevant to the machine learning aspects of this paper (aside from the fact that they produce useful features). The fifth plot in each of the Figures (yellow) shows a distribution of the time difference between consecutive bass hits on the horizontal axis, and their relative frequency of occurrences on the vertical axis. Please note that in the electronic song, there is a large, impulse-like spike at a particular value. This is the near-constant bass-kick that is so common in electronic music. In the classical piece however, there is no such strong bass rhythm. My code computes the magnitude of the peak in this graph, and also measures the average time distance between bass kicks, and uses these values as the final two feature vectors, yielding a grand total of 34 features per feature vector.

So, in summary, here are the 34-dimensional feature vectors used for classification:
\begin{align*}
\phi = \begin{bmatrix}
\text{bass average}	\\
\text{bass spread}	\\
\text{bass peak}	\\
\text{bass max}	\\
\text{bass bin 0}	\\
\text{bass bin 1}	\\
\text{bass bin 2}	\\
\text{bass bin 3}	\\ \\
\text{... repeat above 8 for mids, highs, and level ...} \\ \\
\text{rhythm average}	\\
\text{rhythm peak}	\\
\end{bmatrix}
\end{align*}

where ``bass max'' denotes the largest bass measured (largest value on the horizontal axis of each distribution), ``bass peak'' denote the max value of the histogram (max on the vertical axis), and bass bin 0, 1, 2, 3 denote the raw values in each of the histogram bins.


\section{Training and Test Data}
In terms of training data, I manually sorted 186 songs into the five five genres. The break down is as follows: 23 classical, 26 electronic, 35 hip hop, 49 jazz, 53 rock. I divided this set randomly into two equi-sized subsets of 93 songs each, and used one as my training data and the other as my testing data for cross-validation. Both of these sets remained constant in the SVM and Naive Bayes classifier discussions that follow.

\section{SVM Classification}
Next, after computing the above-mentioned feature vectors, I used an off-the-shelf Support Vector Machine (SVM) multiclass classifier on the data. This package, developed at Cornell University by Thorsten Joachims (see \textrm{http://svmlight.joachims.org/svm\_multiclass.html}), is designed to be an efficient multiclass classifier that functions on linear kernels (non-linear kernels are not fully supported, so I refrained from using them in this project).

\subsection*{Regularization}
The SVM has a number of tunable parameters, one of which is a regularization parameter $C$. With $n$ data points, the quantity $C/n$ is used to weight the sum of the slack variables during SVM training. Therefore, a smaller $C$ will result in less weight being put on the slack, hence yielding a classifier with a larger margin but with higher training error. In contrast, a large $C$ heavily penalizes the use of slack variables, thereby encouraging a smaller margin but a lower training error.

I used the SVM package, and ran the SVM learning algorithm multiple times for different values of $C$ to examine the resulting training and testing errors. The results are plotted in Figure \ref{fig:C_Regularization}. The results make sense from a theoretical standpoint. As noted above, the training error should decrease with increasing $C$. This can be seen in the graph. We also note an interesting pattern for test error. For very small $C$, the test error is larger - this is because the slack variables are being used too much, and the model is henceforth not complex enough to capture the subtle intricacies of the feature vectors that allow accurate classification. As $C$ increases to around 50 or so, we find that the test error reaches a minimum, and as $C$ increases further to very large values, our test error increases again. When $C$ is very large (on the order of 20,000-80,000 in this case), the model is being overfit to the training data. It is too specific to the training set, and henceforth does not generalize well to the cross-validation performed on the test set. The optimal value, with $C$ on the order of a few hundred or thousand, seems to be the best regularization parameter for this classifier and dataset.

\begin{figure}
\includegraphics[scale=0.25]{C_regularization.png}
\caption{Training and test error as a function of regularization parameter C.}
\label{fig:C_Regularization}
\end{figure}


\subsection*{Performance Analysis}
In order to better understand the performance of the classifier, I chose to examine the classification results of one particular trial ($C = 5000$) in greater detail. I was mainly interested in looking for qualitative trends in the prediction errors.

Intuitively before obtaining this data, I expected that hip hop and electronic music would be mixed up and misclassifed as one another (more so than other classes), due to their similarity. Hip hop for example often has strong, regular rhythmic patterns and very deep bass, as does electronic music. Similarly, the distinction between jazz and rock may also be difficult to discern from my feature vectors, which fail to directly capture higher-order information such as instrumentation, musical tonality, etc. However, since such genres as hip hop and classical are quite separable in terms of their bass levels and rhythms, I expected few mistakes between these genres.

My hunch was largely verified, as can be seen in a summary of the results for this trial in Figure \ref{fig:PerfAnalysis}. When a an electronic or hip hop song was misclassified, it was nearly always predicted to be the other genre. Results were similar for jazz and rock. Classical was usually accurate, though songs were occasionally misclassified into other genres.

\begin{figure}
\begin{tabular}{| c || c | c | c | c | c |}
\hline
 & Electronic & Rock & Hip Hop & Classical & Jazz \\ \hline \hline
Electronic & 5 & 1 & 6 & 0 & 0 \\ \hline
Rock & 0 & 13 & 0 & 1 & 9 \\ \hline
Hip Hip & 6 & 2 & 14 & 1 & 0 \\ \hline
Classical & 0 & 1 & 0 & 9 & 0 \\ \hline
Jazz & 0 & 6 & 0 & 1 & 18 \\ \hline
\end{tabular}
\caption{Performance analysis over test set, with $C = 5000$. Horizontal labels represent actual genre, vertical labels represent predicted genre, and the number in each cell indicates the count of how many songs (out of 93) had the given actual and predicted genres.}
\label{fig:PerfAnalysis}
\end{figure}

\section{Naive Bayes Classification}
In addition to the SVM off-the-shelf classifier, I also trained a Naive Bayes multiclass classifier that I designed and implemented in Python (please see Appendix A for the complete source code). I will describe the Naive Bayes classifier and it's classification results in this section, and compare its performance with the SVM classifier in the next section.

A Naive Bayes classifier makes a key assumption about the model data - namely that different features are independent of one another when conditioned on a specific class. This assumption, though not always valid (and most certainly not in our case - as will be discussed later), allows for very efficient training and classification with surprisingly decent results.

Let $y$ denote the different classes (here genres), in $y = 1, 2, ..., k$. Let $X$ denote some feature vector with components $x_1, ..., x_n$. When classifying a point, the Naive Bayes classifier chooses the class yielding the maximal posterior probability:
\begin{align*}
\hat{y} = \text{arg}\max_y p(X, y; \theta) &= \text{arg}\max_y p(x_1, x_2, ..., x_n | y; \theta) p(y; \theta) \\
&= \text{arg}\max_y q(y) \prod_{i = 1}^d {q(x_i | y)}
\end{align*}

where the prior probabilities $q(y)$ and conditional probabilities $q(x_i | y)$, both of which are contained within $\theta$, are computed from the training data. The second equality above is the Naive Bayes assumption. There are multiple ways to model the parameters $q(y)$ and $q(x_i | y)$, one of which is probability tables. These are useful for discrete data, but our machine learning task has continuous feature vectors. One useful and common technique for accommodating continuous data is to model the conditional probabilities $q(x_i | y)$ as one-dimensional Gaussian distributions. This is what I have chosen in my Naive Bayes implementation.

After implementing the classifier, I trained and cross-validated it with the same sets of data I used with the SVM classifier. I found that the performance was comparable, yielding a training error of 24.71\% and a test error of 36.56\%. This is comparable to the SVM classifier with a moderate value of $C$, such as around 5 (which yielded decent SVM performance). 

It is impressive that the Naive Bayes classifier works so well, given that it's core assumption - namely that features are independent conditioned on the class - is most certainly violated in this context. The features are very closely correlated with one another. For example, the bass average is directly related to the values in each of the bass histogram bins. However, even given this inter-dependence of features, the Naive Bayes classifier still performs reasonably well, and seems to generalize across its test set.


\section{Comparison of SVM and Naive Bayes Classification}
The performance of the SVM and Naive Bayes classifier were comparable. The SVM classifier yielded it's lowest test error at $C = 50$ with around 32\%, and the Naive Bayes classifier had a test error of about 37\% (just slightly worse).

In terms of tuning parameters, the SVM classifier offers many more ``knobs'' and possible adjustments (i.e., different kernels, different cost functions for slack variables, the regularization parameter $C$, etc.) than the Naive Bayes classifier, which is largely fixed except for the choice of the conditional probability model (in my case, Gaussian). This affords a great degree of control over the SVM classifier, and to allow for a tradeoff between model complexity versus generalization performance. This is much harder to do with the Naive Bayes classifier. However, the Naive Bayes classifier is advantageous in its extremely computationally efficient training algorithm (much faster in this case than the SVM), and in its simplicity.

I was very interested in seeing the correlation in errors between the SVM and Naive Bayes classifier. Upon examination of the $C = 5000$ case, I found that both classifiers were simultaneously wrong on 18 of the 93 test samples. Of these 18, both classifiers predicted the same (incorrect) genre 15 times. This shows systematic error across classification algorithms, so the feature vectors must not provide enough contrast to linearly separate all of the genres in these examples. In other words, since both the SVM and and the Naive Bayes classifier miss-classify certain examples in the same way, it is likely the feature vectors (and not the algorithms themselves) that have need for improvement.

\section{Conclusions and Future Work}
In conclusion, music genre classification is possible using linear SVM and Naive Bayes classification techniques. Given the frequency-related feature vectors that I computed, cross validation on a test set yielded about 32\% error for the SVM, and about 37\% for the Naive Bayes.

There are many things that I would like to pursue with this project in order to improve performance, the most notable of which is the implementation of more feature vectors. Computing higher-order information about each piece, such as the tempo, more rhythmic information, the tonality (ex., major versus minor), the instrumentation etc. would all likely greatly improve the performance of these classifiers. However, such feature vectors would likely be extremely difficult to compute - and may even involve machine learning algorithms themselves! Nonetheless, improvement of current feature vectors would be a good step towards higher performance.

I would also like to test how this system works outside of the songs in my training and test sets. It is quite possible that the songs used in this study are biased in some way, and even more likely that the prior probabilities of each genre computed by the Naive Bayes classifier are inaccurate (they were based just on my music samples). Spending more time obtaining a greater variety of music and a larger training and testing set would improve performance, especially if combined with improved feature vectors.



	
\pagebreak 
\begin{widetext}
\section*{Appendix A - Source Code}
Please note that I have specifically chosen not to include the Java code that computes the feature vectors, since it mainly consists of signal processing code (i.e., butterfly computations in FFT's, etc.) that are irrelevant to the machine learning aspects of this project.

As the Naive Bayes classifier is on the other hand very relevant, I have included it's Python source code below: 

\begin{verbatim}
'''
Naive Bayes Multiclass Classifier

Programmed by Steve Levine, 2010 as part of his
6.867 Machine Learning Final Project

Classifier:
Let X= feature vector, [x1, x2, ...xd]. d features
Let the classes be denoted 1, 2, ... k.

Output of classifier:
argmax p(X, y; theta) = argmax q(y)*Product( q(x_i | y) )
  y                        y        i = 1:d


'''
import math


'''
Trains a Naive bayes multiclass classifier based on frequencies
measured in the training data. Uses 1D Gaussian distributions to
model the probabilities of individual features (which are of course
assumed to be independent).

Input:
    training_data: tuples of the form [(y1, X1), (y2, X2), ...]
            where yi is the class associated with Xi

Output:
    A dictionary called model, mapping classes to lists of tuples.
        model[k] = [(u1, sigma1), (u2, sigma2), ... (ud, sigmad)]
                where each u and sigma define a Gaussian distribution
                over the corresponding features for that class. This
                is sufficient to define all of independent conditional
                probabilities we'll need.

        Also, model[0] is a special dict that contains the prior
        probabilities over the classes:

        model[0] = {1 => 0.25, ... k => some prob}
'''
def naive_bayes_train(training_data):
    model = {}
    sorted_training_data = sort_training_data_by_classes(training_data)
    d = get_feature_vector_dimension(training_data)

    # Compute the priors - based simply on frequency in the training data
    priors = {}
    N = 0
    for k in sorted_training_data.keys():
        count = len(sorted_training_data[k])
        priors[k] = count
        N = N + count
    for k in sorted_training_data.keys(): # Normalize
        priors[k] = 1.0 * priors[k] / N

    model[0] = priors

    # Compute the conditional distributions, which we model
    # as Gaussian.
    for k in sorted_training_data.keys():
        cond_dist_params_k = []
        for i in range(d):
            x = [X[i] for X in sorted_training_data[k]] # Gets all of feature i for class k
            u = 1.0*sum(x)/len(x)
            sigma = math.sqrt(1.0/(len(x) - 1)*sum([(1.0*xi - u)**2 for xi in x]))
            if (sigma == 0):
                sigma = 0.0001
            cond_dist_params_k.append((u, sigma))

        model[k] = cond_dist_params_k

    return model

'''
Takes in a model and a feature vector, and classifies the point.
'''
def naive_bayes_classify(x, model):
    priors = model[0]
    classes = priors.keys()

    # Compute and return the class with the highest probability
    posteriors = {}
    for k in classes:
        d = len(model[k])
        
        # Compute the posteriors
        posterior = priors[k]
        for i in range(d):
            (u, sigma) = model[k][i]
            posterior = posterior*gaussian_pdf(x[i], u, sigma)

        posteriors[k] = posterior


    # Now, just return the argmax over the posteriors.
    maxPosterior = 0
    maxK = 0
    for k in classes:
        if posteriors[k] > maxPosterior:
            maxPosterior = posteriors[k]
            maxK = k
    return maxK



'''
The following are utility functions
'''

# Returns dictionary of training feature vectors, sorted by
def sort_training_data_by_classes(training_data):
    num_classes = get_number_of_classes(training_data)

    # Initialize
    sorted_data = {}
    for i in range(1, num_classes + 1):
        sorted_data[i] = []
    
    for (y, X) in training_data:
        sorted_data[y].append(X)

    return sorted_data

def get_feature_vector_dimension(training_data):
    # Just look at the first feature vector. Assume they're all the same.
    (y, X) = training_data[0]
    return len(X)

# Returns the number of distinct classes
def get_number_of_classes(training_data):
    max_y = 0
    for (y, X) in training_data:
       max_y = max(max_y, y)
    return max_y

def gaussian_pdf(x, u, sigma):
    return 1/math.sqrt(2*math.pi*sigma**2)*math.exp(-(x - u)**2 / (2*sigma**2))


'''
Reads in a feature vector file, in the same format
as the SVM MultiClass package from Cornell.

Although it's not as general - assumes consecutive, non-skipping
feature vector indices.
'''
def read_training_data_from_filename(filename):
    f = open(filename, "r")
    lines = f.readlines()
    f.close()

    training_data = []
    for line in lines:
        line = line.strip()
        fields = line.split(" ")

        y = int(fields[0]) # The class
        X = []
        for i in range(1, len(fields)):
            X.append(float(fields[i].split(":")[1]))

        training_data.append((y, X))

    return training_data

'''
Do some analysis
'''
def error_analysis(model, test_data):
    results = []

    correct = 0
    N = 0
    
    for (y, X) in test_data:
        y_hat = naive_bayes_classify(X, model)
        results.append((y, y_hat))
        if y == y_hat:
            correct = correct + 1
        N = N + 1

    # Write the results to a file
    lines = []
    genres = {}
    genres[1] = "Electronic"
    genres[2] = "Rock"
    genres[3] = "Classical"
    genres[4] = "Jazz"
    genres[5] = "HipHop"
    for (y, y_hat) in results:
        lines.append(genres[y] + "," + genres[y_hat] + "\n")
    f = open("/home/steve/Desktop/Music/Naive Bayes Classifier/results.csv","w")
    f.writelines(lines)
    f.close()

    print "Correct: %f" % (1.0*correct/N)

\end{verbatim}


\end{widetext}
\end{document}